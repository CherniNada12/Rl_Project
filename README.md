# ğŸ§  Apprentissage par Renforcement - Jeu du TrÃ©sor

Application web interactive pour visualiser et comparer diffÃ©rents algorithmes d'apprentissage par renforcement (Q-Learning, Double Q-Learning, SARSA).

## ğŸ“ Architecture du Projet
```
RLWeb/
â”œâ”€â”€ app.py              # Point d'entrÃ©e Flask
â”œâ”€â”€ environment.py      # Logique de la grille
â”œâ”€â”€ agent.py            # Logique de l'agent RL
â”œâ”€â”€ algorithms.py       # Q-Learning, Double Q-Learning, SARSA
â”œâ”€â”€ trainer.py          # EntraÃ®nement de l'agent
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html      # Interface utilisateur (HTML)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css       # Styles CSS
â”‚   â””â”€â”€ script.js       # Logique front-end (JavaScript)
â””â”€â”€ README.md           # Documentation
```
## ğŸ–¼ï¸ Interface de lâ€™application

![Interface du Jeu du TrÃ©sor](interface.png)

## ğŸ¤– Mode dÃ©monstration

![DÃ©monstration du Jeu du TrÃ©sor](demenstration.png)


## ğŸ¥ DÃ©monstration vidÃ©o

ğŸ‘‰ [Cliquez ici pour voir la dÃ©monstration](Projet_Rl_Demo.mp4)


## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+
- pip

### Ã‰tapes

1. **Cloner le repository**
```bash
git clone <[votre-repo](https://github.com/CherniNada12/Rl_Project.git)>
cd RLWeb
```

2. **CrÃ©er un environnement virtuel**
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

3. **Installer les dÃ©pendances**
```bash
pip install flask numpy
```

4. **Lancer l'application**
```bash
python app.py
```

5. **AccÃ©der Ã  l'application**
Ouvrir votre navigateur Ã  l'adresse : `http://localhost:5000`

## ğŸ“š Modules

### `environment.py`
GÃ¨re l'environnement de la grille :
- CrÃ©ation de la grille avec obstacles, piÃ¨ges et trÃ©sor
- Calcul des rÃ©compenses
- VÃ©rification des Ã©tats terminaux

### `agent.py`
ImplÃ©mente l'agent d'apprentissage :
- Gestion des Q-tables (simple et double)
- Politique epsilon-greedy
- Calcul des transitions d'Ã©tat

### `algorithms.py`
Contient les algorithmes d'apprentissage :
- **Q-Learning** : Algorithme off-policy
- **Double Q-Learning** : RÃ©duit la surestimation des valeurs Q
- **SARSA** : Algorithme on-policy

### `trainer.py`
GÃ¨re le processus d'entraÃ®nement :
- ExÃ©cution des Ã©pisodes
- Collecte des statistiques
- DÃ©monstration de l'agent entraÃ®nÃ©

### `app.py`
Serveur Flask avec les endpoints API :
- `/` : Page principale
- `/train` : EntraÃ®ner un Ã©pisode
- `/reset` : RÃ©initialiser l'agent
- `/demonstrate` : DÃ©monstration du chemin optimal

## ğŸ® Utilisation

1. **Choisir un algorithme** : Q-Learning, Double Q-Learning ou SARSA
2. **Ajuster les paramÃ¨tres** :
   - Î± (alpha) : Taux d'apprentissage
   - Î³ (gamma) : Facteur de rÃ©duction
   - Îµ decay : DÃ©croissance epsilon
3. **DÃ©marrer l'entraÃ®nement** : Observer l'agent apprendre
4. **DÃ©monstration** : Voir le chemin optimal trouvÃ©
5. **RÃ©initialiser** : Recommencer l'entraÃ®nement

## ğŸ¯ FonctionnalitÃ©s

- âœ… 3 algorithmes d'apprentissage par renforcement
- âœ… Visualisation en temps rÃ©el de l'entraÃ®nement
- âœ… Statistiques dÃ©taillÃ©es (taux de succÃ¨s, rÃ©compense moyenne, etc.)
- âœ… Affichage des Q-values sur la grille
- âœ… Mode dÃ©monstration pour voir le chemin optimal
- âœ… Interface responsive et intuitive

## ğŸ“Š Environnement

La grille 8Ã—8 contient :
- ğŸ¤– **Agent** : Position de dÃ©part (0, 7)
- ğŸ’ **TrÃ©sor** : Objectif Ã  atteindre (+100 points)
- ğŸ§± **Murs** : Obstacles infranchissables (-10 points)
- âš ï¸ **PiÃ¨ges** : Zones dangereuses (-50 points)
- ğŸ **DÃ©part** : Case de dÃ©part

## ğŸ”§ Technologies UtilisÃ©es

- **Backend** : Flask (Python)
- **Frontend** : HTML5, CSS3, JavaScript
- **ML** : NumPy (calculs matriciels)
- **Algorithmes** : Q-Learning, Double Q-Learning, SARSA

## ğŸ§  Algorithmes implÃ©mentÃ©s

### ğŸ”¹ Q-Learning (off-policy)

Le Q-Learning apprend la politique optimale indÃ©pendamment de la politique suivie par lâ€™agent.

**Ã‰quation de mise Ã  jour :**
Q(s,a) â† Q(s,a) + Î± [ r + Î³ maxâ‚â€² Q(sâ€²,aâ€²) âˆ’ Q(s,a) ]



### ğŸ”¹ Double Q-Learning (off-policy)

Le Double Q-Learning est une amÃ©lioration du Q-Learning classique visant Ã  rÃ©duire la surestimation des valeurs Q.

**Ã‰quation de mise Ã  jour :**
Q1(s,a) â† Q1(s,a) + Î± [ r + Î³ Q2(sâ€², argmax Q1(sâ€²,aâ€²)) âˆ’ Q1(s,a) ]
Q2(s,a) â† Q2(s,a) + Î± [ r + Î³ Q1(sâ€², argmax Q2(sâ€²,aâ€²)) âˆ’ Q2(s,a) ]

### ğŸ”¹  SARSA (on-policy)

SARSA (Stateâ€“Actionâ€“Rewardâ€“Stateâ€“Action) est un algorithme on-policy qui apprend la valeur des actions rÃ©ellement suivies par la politique courante.

**Ã‰quation de mise Ã  jour :**
Q(s,a) â† Q(s,a) + Î± [ r + Î³ Q(sâ€²,aâ€²) âˆ’ Q(s,a) ]

## ğŸ“Š Tableau comparatif des algorithmes dâ€™apprentissage par renforcement

| CritÃ¨re | **Q-Learning** | **Double Q-Learning** | **SARSA** |
|--------|---------------|----------------------|-----------|
| **Type** | Off-policy | Off-policy | On-policy |
| **IdÃ©e clÃ©** | Apprend via la meilleure action possible | RÃ©duit la surestimation avec deux tables Q | Apprend selon la politique suivie |
| **Mise Ã  jour** | `max Q(sâ€²,aâ€²)` | `Q1 / Q2 alternÃ©es` | `Q(sâ€²,aâ€²)` |
| **Surestimation** | Ã‰levÃ©e âš ï¸ | Faible âœ… | Faible âœ… |
| **StabilitÃ©** | Moyenne | Ã‰levÃ©e | Ã‰levÃ©e |
| **Convergence** | Rapide | Moyenne | Plus lente |
| **Comportement** | RisquÃ© | Ã‰quilibrÃ© | Prudent |
| **MÃ©moire** | 1 table Q | 2 tables Q | 1 table Q |
| **RÃ©sultat (Jeu du TrÃ©sor)** | Rapide mais instable | Stable et fiable | SÃ»r mais plus long |


## ğŸ‘¨â€ğŸ’» Auteur

Nada Cherni & Maysen Chiha 

