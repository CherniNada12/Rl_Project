# ğŸ§  Apprentissage par Renforcement - Jeu du TrÃ©sor

Application web interactive pour visualiser et comparer diffÃ©rents algorithmes d'apprentissage par renforcement (Q-Learning, Double Q-Learning, SARSA).

## ğŸ“ Architecture du Projet
```
RLWeb/
â”œâ”€â”€ app.py              # Point d'entrÃ©e Flask
â”œâ”€â”€ environment.py      # Logique de la grille
â”œâ”€â”€ agent.py            # Logique de l'agent RL
â”œâ”€â”€ algorithms.py       # Q-Learning, Double Q-Learning, SARSA
â”œâ”€â”€ trainer.py          # EntraÃ®nement de l'agent
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html      # Interface utilisateur (HTML)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css       # Styles CSS
â”‚   â””â”€â”€ script.js       # Logique front-end (JavaScript)
â””â”€â”€ README.md           # Documentation
```

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+
- pip

### Ã‰tapes

1. **Cloner le repository**
```bash
git clone <votre-repo>
cd RLWeb
```

2. **CrÃ©er un environnement virtuel**
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

3. **Installer les dÃ©pendances**
```bash
pip install flask numpy
```

4. **Lancer l'application**
```bash
python app.py
```

5. **AccÃ©der Ã  l'application**
Ouvrir votre navigateur Ã  l'adresse : `http://localhost:5000`

## ğŸ“š Modules

### `environment.py`
GÃ¨re l'environnement de la grille :
- CrÃ©ation de la grille avec obstacles, piÃ¨ges et trÃ©sor
- Calcul des rÃ©compenses
- VÃ©rification des Ã©tats terminaux

### `agent.py`
ImplÃ©mente l'agent d'apprentissage :
- Gestion des Q-tables (simple et double)
- Politique epsilon-greedy
- Calcul des transitions d'Ã©tat

### `algorithms.py`
Contient les algorithmes d'apprentissage :
- **Q-Learning** : Algorithme off-policy
- **Double Q-Learning** : RÃ©duit la surestimation des valeurs Q
- **SARSA** : Algorithme on-policy

### `trainer.py`
GÃ¨re le processus d'entraÃ®nement :
- ExÃ©cution des Ã©pisodes
- Collecte des statistiques
- DÃ©monstration de l'agent entraÃ®nÃ©

### `app.py`
Serveur Flask avec les endpoints API :
- `/` : Page principale
- `/train` : EntraÃ®ner un Ã©pisode
- `/reset` : RÃ©initialiser l'agent
- `/demonstrate` : DÃ©monstration du chemin optimal

## ğŸ® Utilisation

1. **Choisir un algorithme** : Q-Learning, Double Q-Learning ou SARSA
2. **Ajuster les paramÃ¨tres** :
   - Î± (alpha) : Taux d'apprentissage
   - Î³ (gamma) : Facteur de rÃ©duction
   - Îµ decay : DÃ©croissance epsilon
3. **DÃ©marrer l'entraÃ®nement** : Observer l'agent apprendre
4. **DÃ©monstration** : Voir le chemin optimal trouvÃ©
5. **RÃ©initialiser** : Recommencer l'entraÃ®nement

## ğŸ¯ FonctionnalitÃ©s

- âœ… 3 algorithmes d'apprentissage par renforcement
- âœ… Visualisation en temps rÃ©el de l'entraÃ®nement
- âœ… Statistiques dÃ©taillÃ©es (taux de succÃ¨s, rÃ©compense moyenne, etc.)
- âœ… Affichage des Q-values sur la grille
- âœ… Mode dÃ©monstration pour voir le chemin optimal
- âœ… Interface responsive et intuitive

## ğŸ“Š Environnement

La grille 8Ã—8 contient :
- ğŸ¤– **Agent** : Position de dÃ©part (0, 7)
- ğŸ’ **TrÃ©sor** : Objectif Ã  atteindre (+100 points)
- ğŸ§± **Murs** : Obstacles infranchissables (-10 points)
- âš ï¸ **PiÃ¨ges** : Zones dangereuses (-50 points)
- ğŸ **DÃ©part** : Case de dÃ©part

## ğŸ”§ Technologies UtilisÃ©es

- **Backend** : Flask (Python)
- **Frontend** : HTML5, CSS3, JavaScript
- **ML** : NumPy (calculs matriciels)
- **Algorithmes** : Q-Learning, Double Q-Learning, SARSA

## ğŸ“ Licence

MIT License

## ğŸ‘¨â€ğŸ’» Auteur

Votre Nom - [GitHub](https://github.com/votre-username)

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou un pull request.
